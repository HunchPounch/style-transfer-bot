{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CycleGAN pretraining"
      ],
      "metadata": {
        "id": "1rolcrEa6E_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_fid"
      ],
      "metadata": {
        "id": "976-XWLdB6ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BZViEP4X5B81"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import argparse\n",
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pytorch_fid import fid_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка Датасета"
      ],
      "metadata": {
        "id": "aEZqdNuU9F2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/vangogh2photo.zip\"\n",
        "!unzip -q vangogh2photo.zip -d dataset"
      ],
      "metadata": {
        "id": "jUo0zqyV6DXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "511e019e-13ed-4ba4-fc4f-cb08d7057721"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-08 14:33:30--  http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/vangogh2photo.zip\n",
            "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.244.190\n",
            "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.244.190|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 306590349 (292M) [application/zip]\n",
            "Saving to: ‘vangogh2photo.zip’\n",
            "\n",
            "vangogh2photo.zip   100%[===================>] 292.39M  4.98MB/s    in 73s     \n",
            "\n",
            "2024-10-08 14:34:44 (4.00 MB/s) - ‘vangogh2photo.zip’ saved [306590349/306590349]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Вспомогательные функции\n",
        "\n",
        "def convert_to_rgb(image):\n",
        "    rgb_image = Image.new(\"RGB\", image.size)\n",
        "    rgb_image.paste(image)\n",
        "    return rgb_image\n",
        "\n",
        "def show_img(img,size=10):\n",
        "  img = img / 2 + 0.5\n",
        "  npimg = img.numpy()\n",
        "  plt.figure(figsize=(size, size))\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.show()\n",
        "\n",
        "def plot_output(path, x, y):\n",
        "    img = mpimg.imread(path)\n",
        "    plt.figure(figsize=(x,y))\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aBxsn4RVCi2H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_) # применяем необходимые трансформации\n",
        "        self.unaligned = unaligned\n",
        "        self.root_A = root[0]\n",
        "        self.root_B = root[1]\n",
        "\n",
        "        self.files_A = sorted(glob.glob(self.root_A + \"/*.*\"))\n",
        "        self.files_B = sorted(glob.glob(self.root_B + \"/*.*\"))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
        "\n",
        "        if self.unaligned:\n",
        "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
        "        else:\n",
        "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
        "\n",
        "        if image_A.mode != \"RGB\":\n",
        "            image_A = convert_to_rgb(image_A)\n",
        "        if image_B.mode != \"RGB\":\n",
        "            image_B = convert_to_rgb(image_B)\n",
        "\n",
        "        item_A = self.transform(image_A)\n",
        "        item_B = self.transform(image_B)\n",
        "\n",
        "        # Возвращаем словарь\n",
        "        return {\"A\": item_A, \"B\": item_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.files_A), len(self.files_B))"
      ],
      "metadata": {
        "id": "TqvaTdOMC8wS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Модель CycleGAN\n",
        "Будем строить архитектуру на основе ResNet"
      ],
      "metadata": {
        "id": "YnFQRv4RGkAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для поддержания разнообразия и эффективности обучения, будем использовать буфер изображений\n",
        "\n",
        "Когда буфер заполнен, для каждого нового изображения сгенерированного моделью:\n",
        "\n",
        " 1) Либо добавляется новое изображение, заменяя одно из старых с вероятностью 0.5.\n",
        "\n",
        " 2) Либо возвращается одно из старых изображений.\n",
        "\n",
        "Таким образом, буфер помогает обучению генеративной модели стабилизироваться и не зацикливаться на одном и том же наборе изображений"
      ],
      "metadata": {
        "id": "m1tQYnunHSMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=50):\n",
        "        assert max_size > 0, \"Empty buffer.\"\n",
        "        self.max_size = max_size\n",
        "        self.data = []\n",
        "\n",
        "    def push_and_pop(self, data):\n",
        "        to_return = []\n",
        "        for element in data.data:\n",
        "            element = torch.unsqueeze(element, 0)\n",
        "            if len(self.data) < self.max_size:\n",
        "                self.data.append(element)\n",
        "                to_return.append(element)\n",
        "            else:\n",
        "                # если больше 0.5 возвращаем новое\n",
        "                if random.uniform(0, 1) > 0.5:\n",
        "                    i = random.randint(0, self.max_size - 1)\n",
        "                    to_return.append(self.data[i].clone())\n",
        "                    self.data[i] = element  # заменяем старое\n",
        "                else:\n",
        "                    # иначе отправляем старое\n",
        "                    to_return.append(element)\n",
        "        return Variable(torch.cat(to_return))"
      ],
      "metadata": {
        "id": "odafSHK6FUOf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scheduler\n",
        "class LambdaLR:\n",
        "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "        if (n_epochs - decay_start_epoch) < 0:\n",
        "            raise Exception(\"Decay should start before training ends.\"\n",
        "                            \"Change decay_start_epoch to a value less than {}.\".format(n_epochs))\n",
        "        self.n_epochs = n_epochs\n",
        "        self.offset = offset\n",
        "        self.decay_start_epoch = decay_start_epoch\n",
        "\n",
        "    def step(self, epoch):\n",
        "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
      ],
      "metadata": {
        "id": "cs71lWwvIMOr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определим главный строительный блок"
      ],
      "metadata": {
        "id": "Bxucw1YFJeA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1), # лучше zero padding\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(channels), # нормализуем не по батчам\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x) # прокидываем изображение через блок"
      ],
      "metadata": {
        "id": "_lO6flT5JdUJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Генератор с backbone ResNet"
      ],
      "metadata": {
        "id": "Zhn7-cunKq5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, input_shape, num_residual_blocks):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        channels = input_shape[0]\n",
        "\n",
        "        # начальный convolution block\n",
        "        out_channels = 64\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(channels),\n",
        "            nn.Conv2d(channels, out_channels, 7),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        ]\n",
        "        in_channels = out_channels\n",
        "\n",
        "        # Encoder\n",
        "        for _ in range(2):\n",
        "            out_channels *= 2\n",
        "            model += [\n",
        "                nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_channels),\n",
        "                nn.LeakyReLU(inplace=True),\n",
        "            ]\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Residual блоки, усложняем представление\n",
        "\n",
        "        for _ in range(num_residual_blocks):\n",
        "            model += [ResNetBlock(out_channels)]\n",
        "\n",
        "        # Decoder\n",
        "        for _ in range(2):\n",
        "            out_channels //= 2\n",
        "            model += [\n",
        "                nn.Upsample(scale_factor=2),\n",
        "                nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),\n",
        "                nn.InstanceNorm2d(out_channels),\n",
        "                nn.LeakyReLU(inplace=True),\n",
        "            ]\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Output layer\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(channels),\n",
        "            nn.Conv2d(out_channels, channels, 7),\n",
        "            nn.Tanh(), # для выходов от -1 до 1\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "EMAHl0kOKmYN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PatchGAN Discriminator\n",
        "PatchGAN проверяет не все изображение целиком, а отдельные участки изображения (patches). На основе этих участков дискриминатор принимает решение о том, является ли каждый участок \"реальным\" или \"сгенерированным\"."
      ],
      "metadata": {
        "id": "2PZD26ScRqOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        channels, height, width = input_shape\n",
        "\n",
        "        # вычисляем выходную размерность PatchGAN\n",
        "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
        "\n",
        "        def discriminator_block(in_channels, out_channels, normalize=True):\n",
        "            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_channels))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        # C64 -> C128 -> C256 -> C512\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(channels, out_channels=64, normalize=False),\n",
        "            *discriminator_block(64, out_channels=128),\n",
        "            *discriminator_block(128, out_channels=256),\n",
        "            *discriminator_block(256, out_channels=512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, padding=1) # каждый пиксель теперь соответсвует патчу\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ],
      "metadata": {
        "id": "DybsT-8xQLMj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Зададим гиперпараметры"
      ],
      "metadata": {
        "id": "tv5NnRHqVhav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Hyperparameters(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.__class__) + \": \" + str(self.__dict__)"
      ],
      "metadata": {
        "id": "HKNlPB3WYtT3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    \"name\": \"CycleGan_VanGogh_Checkpoint\",  # Название модели\n",
        "    \"n_epochs\": 50,  # Количество эпох\n",
        "    \"batch_size\": 4,  # Размер батча\n",
        "    \"lr\": 0.0002,  # learning rate\n",
        "    \"decay_start_epoch\": 5,  # Эпоха, с которой начинается уменьшение скорости обучения\n",
        "    \"b1\": 0.5,  # Параметр для Adam оптимизатора (beta1)\n",
        "    \"b2\": 0.999,  # Параметр для Adam оптимизатора (beta2)\n",
        "    \"img_size\": 256,  # Размер изображения (например, 256x256)\n",
        "    \"channels\": 3,  # Количество каналов\n",
        "    \"num_residual_blocks\": 9,\n",
        "    \"lambda_cyc\": 10.0,  # Взвешивающий коэффициент для циклической потери\n",
        "    \"lambda_id\": 5.0,  # Взвешивающий коэффициент для идентификационной потери\n",
        "    \"data_dir_A\": \"dataset/vangogh2photo/trainA\",  # Путь к данным A\n",
        "    \"data_dir_B\": \"dataset/vangogh2photo/trainB\",  # Путь к данным B\n",
        "    \"val_data_dir_A\": \"dataset/vangogh2photo/testA\",  # Путь к валидационным данным A\n",
        "    \"val_data_dir_B\": \"dataset/vangogh2photo/testB\"  # Путь к валидационным данным B\n",
        "}\n",
        "\n",
        "hp = Hyperparameters(**hyperparameters)\n",
        "print(\"Hyperparameters: \\n\")\n",
        "print(hp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0r4NU-lYyDb",
        "outputId": "8430a6bb-9ba2-4c0b-9b3e-ab4054125b43"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: \n",
            "\n",
            "<class '__main__.Hyperparameters'>: {'name': 'CycleGan_VanGogh_Checkpoint', 'n_epochs': 50, 'batch_size': 4, 'lr': 0.0002, 'decay_start_epoch': 5, 'b1': 0.5, 'b2': 0.999, 'img_size': 256, 'channels': 3, 'num_residual_blocks': 9, 'lambda_cyc': 10.0, 'lambda_id': 5.0, 'data_dir_A': 'dataset/vangogh2photo/trainA', 'data_dir_B': 'dataset/vangogh2photo/trainB', 'val_data_dir_A': 'dataset/vangogh2photo/testA', 'val_data_dir_B': 'dataset/vangogh2photo/testB'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка моделей"
      ],
      "metadata": {
        "id": "DQiqWgf2eOkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms_ = [\n",
        "        transforms.Resize((286, 286)),\n",
        "        transforms.RandomRotation(degrees=(0,180)),\n",
        "        transforms.RandomCrop(size=(hp.img_size,hp.img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ]\n",
        "\n",
        "val_transforms_ = [\n",
        "    transforms.Resize((hp.img_size, hp.img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "]"
      ],
      "metadata": {
        "id": "QMZsCk4HZy6D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(\n",
        "        ImageDataset(root=[hp.data_dir_A,hp.data_dir_B], transforms_=train_transforms_),\n",
        "        batch_size=hp.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2)\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    ImageDataset(root= [hp.val_data_dir_A,hp.val_data_dir_B], transforms_=val_transforms_),\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=2)\n",
        "\n",
        "def to_img(x):\n",
        "    x = x.view(x.size(0)*2, hp.channels, hp.img_size, hp.img_size)\n",
        "    return x"
      ],
      "metadata": {
        "id": "dSXFyrBidgq8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True if torch.cuda.is_available() else False\n",
        "print(\"Using CUDA\" if cuda else \"Not using CUDA\")\n",
        "if cuda is False:\n",
        "    exit(\"CUDA is necessary to train the model.\")\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlKnt1KHeCC8",
        "outputId": "4e908be6-8719-49a1-c786-a5aaadd1534e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_cycle = torch.nn.L1Loss() # побуждаем генератор сохранить как можно больше исходных деталей\n",
        "criterion_identity = torch.nn.L1Loss() # помогает генератору не искажать данные, если они уже принадлежат нужному домену"
      ],
      "metadata": {
        "id": "cDY9vm90eYr_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (hp.channels, hp.img_size, hp.img_size)\n",
        "\n",
        "# инициализируем generator и discriminator\n",
        "Gen_AB = GeneratorResNet(input_shape, hp.num_residual_blocks)\n",
        "Gen_BA = GeneratorResNet(input_shape, hp.num_residual_blocks)\n",
        "Disc_A = Discriminator(input_shape)\n",
        "Disc_B = Discriminator(input_shape)\n",
        "\n",
        "if cuda:\n",
        "    Gen_AB = nn.DataParallel(Gen_AB)\n",
        "    Gen_AB = Gen_AB.cuda()\n",
        "    Gen_BA = nn.DataParallel(Gen_BA)\n",
        "    Gen_BA = Gen_BA.cuda()\n",
        "    Disc_A = nn.DataParallel(Disc_A)\n",
        "    Disc_A = Disc_A.cuda()\n",
        "    Disc_B = nn.DataParallel(Disc_B)\n",
        "    Disc_B = Disc_B.cuda()\n",
        "    criterion_GAN.cuda()\n",
        "    criterion_cycle.cuda()\n",
        "    criterion_identity.cuda()\n",
        "\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()"
      ],
      "metadata": {
        "id": "Vmr_oAuMeh3C"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_G = torch.optim.Adam(itertools.chain(Gen_AB.parameters(),\n",
        "                                               Gen_BA.parameters()),\n",
        "                               lr=hp.lr,\n",
        "                               betas=(hp.b1, hp.b2))\n",
        "\n",
        "optimizer_Disc_A = torch.optim.Adam(Disc_A.parameters(), lr=hp.lr, betas=(hp.b1, hp.b2))\n",
        "optimizer_Disc_B = torch.optim.Adam(Disc_B.parameters(), lr=hp.lr, betas=(hp.b1, hp.b2))\n",
        "\n",
        "# schedulers\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_G, lr_lambda=LambdaLR(hp.n_epochs, 0, hp.decay_start_epoch).step)\n",
        "lr_scheduler_Disc_A = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_Disc_A, lr_lambda=LambdaLR(hp.n_epochs, 0, hp.decay_start_epoch).step)\n",
        "lr_scheduler_Disc_B = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_Disc_B, lr_lambda=LambdaLR(hp.n_epochs, 0, hp.decay_start_epoch).step)"
      ],
      "metadata": {
        "id": "99b2J0KxewtR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"checkpoint\\CycleGan_VanGogh_Checkpoint.pt\") if os.path.exists(\"checkpoint\\CycleGan_VanGogh_Checkpoint.pt\") else None"
      ],
      "metadata": {
        "id": "7XtXD6KQfHUg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_conv_weights_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if hasattr(m, \"bias\") and m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)"
      ],
      "metadata": {
        "id": "kTEme6lDfaWV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if checkpoint is not None:\n",
        "    print(\"Loading checkpoint...\")\n",
        "    Gen_AB.load_state_dict(checkpoint['Gen_AB'])\n",
        "    Gen_BA.load_state_dict(checkpoint['Gen_BA'])\n",
        "    Disc_A.load_state_dict(checkpoint['Disc_A'])\n",
        "    Disc_B.load_state_dict(checkpoint['Disc_A'])\n",
        "    optimizer_G.load_state_dict(checkpoint['optimizer_G'])\n",
        "    optimizer_Disc_A.load_state_dict(checkpoint['optimizer_Disc_A'])\n",
        "    optimizer_Disc_B.load_state_dict(checkpoint['optimizer_Disc_B'])\n",
        "    print(\"Successfully loaded checkpoint.\")\n",
        "else:\n",
        "    # инициализируем веса\n",
        "    Gen_AB.apply(initialize_conv_weights_normal)\n",
        "    Gen_BA.apply(initialize_conv_weights_normal)\n",
        "    Disc_A.apply(initialize_conv_weights_normal)\n",
        "    Disc_B.apply(initialize_conv_weights_normal)"
      ],
      "metadata": {
        "id": "RTUSYguLfLSZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train loop"
      ],
      "metadata": {
        "id": "VSxO2Mehfl-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_img_samples(epoch):\n",
        "    # сохраняем изображения каждую эпоху\n",
        "    Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "    imgs = next(iter(val_dataloader))\n",
        "    Gen_AB.eval()\n",
        "    Gen_BA.eval()\n",
        "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
        "    fake_B = Gen_AB(real_A)\n",
        "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
        "    fake_A = Gen_BA(real_B)\n",
        "\n",
        "    # строим сетку из изображений\n",
        "    real_A = make_grid(real_A, nrow=16, normalize=True)\n",
        "    real_B = make_grid(real_B, nrow=16, normalize=True)\n",
        "    fake_A = make_grid(fake_A, nrow=16, normalize=True)\n",
        "    fake_B = make_grid(fake_B, nrow=16, normalize=True)\n",
        "\n",
        "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
        "    path =  \"outputs-{}.png\".format(epoch)\n",
        "\n",
        "    save_image(image_grid, path, normalize=False)\n",
        "    return path"
      ],
      "metadata": {
        "id": "84QhQ5tpGfbo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(name,Gen_BA,Gen_AB,Disc_A,Disc_B,train_dataloader,n_epochs,criterion_identity,\n",
        "          criterion_cycle,lambda_cyc,criterion_GAN,optimizer_G,fake_A_buffer,fake_B_buffer,\n",
        "          optimizer_Disc_A,optimizer_Disc_B,Tensor,lambda_id):\n",
        "\n",
        "    disc_loss = 0\n",
        "    gen_loss = 0\n",
        "    id_loss = 0\n",
        "    disc_loss_total,gen_loss_total, id_loss_total = [],[],[]\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch in tqdm(train_dataloader):\n",
        "\n",
        "\n",
        "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
        "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
        "\n",
        "            # ground truths\n",
        "            valid = Variable(\n",
        "                Tensor(np.ones((real_A.size(0), *Disc_A.module.output_shape))),\n",
        "                requires_grad=False,\n",
        "            )\n",
        "            fake = Variable(\n",
        "                Tensor(np.zeros((real_A.size(0), *Disc_A.module.output_shape))),\n",
        "                requires_grad=False,\n",
        "            )\n",
        "\n",
        "            #########################\n",
        "            #  Train Generators\n",
        "            #########################\n",
        "\n",
        "            Gen_AB.module.train() # Gen_AB(real_A) возьмет real_A и создаст fake_B\n",
        "            Gen_BA.module.train() # Gen_BA(real_B) возьмет real_B и создаст fake_A\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Identity loss\n",
        "            loss_id_A = criterion_identity(Gen_BA(real_A), real_A)\n",
        "\n",
        "            loss_id_B = criterion_identity(Gen_AB(real_B), real_B)\n",
        "\n",
        "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
        "            id_loss += loss_identity.item()\n",
        "\n",
        "            # лосс для GAN_AB\n",
        "            fake_B = Gen_AB(real_A)\n",
        "            loss_GAN_AB = criterion_GAN(Disc_B(fake_B), valid)\n",
        "\n",
        "            # лосс для GAN_BA\n",
        "            fake_A = Gen_BA(real_B)\n",
        "            loss_GAN_BA = criterion_GAN(Disc_A(fake_A), valid)\n",
        "\n",
        "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "\n",
        "            # Cycle Consistency losses\n",
        "            reconstructed_A = Gen_BA(fake_B)\n",
        "\n",
        "            loss_cycle_A = criterion_cycle(reconstructed_A, real_A)\n",
        "\n",
        "            reconstructed_B = Gen_AB(fake_A)\n",
        "\n",
        "            loss_cycle_B = criterion_cycle(reconstructed_B, real_B)\n",
        "\n",
        "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
        "\n",
        "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
        "            gen_loss+=loss_G.item()\n",
        "\n",
        "            loss_G.backward()\n",
        "\n",
        "            optimizer_G.step()\n",
        "\n",
        "            #########################\n",
        "            #  Train Discriminator A\n",
        "            #########################\n",
        "\n",
        "            optimizer_Disc_A.zero_grad()\n",
        "\n",
        "            # Real loss\n",
        "            loss_real = criterion_GAN(Disc_A(real_A), valid)\n",
        "            # Fake loss по батчу сохраненных ранее\n",
        "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
        "            loss_fake = criterion_GAN(Disc_A(fake_A_.detach()), fake)\n",
        "\n",
        "            loss_Disc_A = (loss_real + loss_fake) / 2\n",
        "            # disc_loss_A += loss_Disc_A.item()\n",
        "            loss_Disc_A.backward()\n",
        "\n",
        "            optimizer_Disc_A.step()\n",
        "\n",
        "            #########################\n",
        "            #  Train Discriminator B\n",
        "            #########################\n",
        "\n",
        "            optimizer_Disc_B.zero_grad()\n",
        "\n",
        "            # Real loss\n",
        "            loss_real = criterion_GAN(Disc_B(real_B), valid)\n",
        "            # Fake loss по батчу сохраненных ранее\n",
        "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
        "            loss_fake = criterion_GAN(Disc_B(fake_B_.detach()), fake)\n",
        "            loss_Disc_B = (loss_real + loss_fake) / 2\n",
        "\n",
        "            loss_Disc_B.backward()\n",
        "\n",
        "            optimizer_Disc_B.step()\n",
        "\n",
        "            loss_D = (loss_Disc_A + loss_Disc_B) / 2\n",
        "            disc_loss+= loss_D.item()\n",
        "\n",
        "        gen_loss = gen_loss/len(train_dataloader)\n",
        "        disc_loss = disc_loss/len(train_dataloader)\n",
        "        id_loss = id_loss/len(train_dataloader)\n",
        "        gen_loss_total.append(gen_loss)\n",
        "        disc_loss_total.append(disc_loss)\n",
        "        id_loss_total.append(id_loss)\n",
        "        plot_output(save_img_samples(epoch), 30, 40)\n",
        "\n",
        "        path = \"./checkpoint\"\n",
        "        if os.path.exists(path) is not True:\n",
        "            os.mkdir(path)\n",
        "        path = path + \"/\"+name+\".pt\"\n",
        "        torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'Gen_AB': Gen_AB.state_dict(),\n",
        "                    'Gen_BA': Gen_BA.state_dict(),\n",
        "                    'Disc_A': Disc_A.state_dict(),\n",
        "                    'Disc_B': Disc_B.state_dict(),\n",
        "                    'optimizer_G': optimizer_G.state_dict(),\n",
        "                    'optimizer_Disc_A': optimizer_Disc_A.state_dict(),\n",
        "                    'optimizer_Disc_B': optimizer_Disc_B.state_dict()}, path)\n",
        "        print(\n",
        "                \"\\r[Epoch %d/%d] [Disc loss: %f] [Gen loss: %f] [Identity loss: %f]\"\n",
        "                % (\n",
        "                    epoch+1,\n",
        "                    n_epochs,\n",
        "                    disc_loss,\n",
        "                    gen_loss,\n",
        "                    id_loss,\n",
        "                )\n",
        "            )\n",
        "    losses = {\"gen_loss\": gen_loss_total,\"disc_loss\": disc_loss_total,\"id_loss\": id_loss_total}\n",
        "    #with open('outputs/losses.pickle', 'wb') as handle:\n",
        "    #    pickle.dump(losses, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(gen_loss_total, label=\"Generator Loss\")\n",
        "    plt.title(\"Generator Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(disc_loss_total, label=\"Discriminator Loss\")\n",
        "    plt.title(\"Discriminator Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(id_loss_total, label=\"ID Loss\")\n",
        "    plt.title(\"ID Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6QBIts9XfRGD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(name = hp.name, Gen_BA = Gen_BA,Gen_AB = Gen_AB,Disc_A = Disc_A,Disc_B = Disc_B,train_dataloader = train_dataloader,\n",
        "          n_epochs = hp.n_epochs,criterion_identity = criterion_identity,criterion_cycle = criterion_cycle, lambda_cyc = hp.lambda_cyc,\n",
        "          criterion_GAN = criterion_GAN,optimizer_G = optimizer_G,fake_A_buffer = fake_A_buffer,fake_B_buffer = fake_B_buffer,\n",
        "          optimizer_Disc_A = optimizer_Disc_A,optimizer_Disc_B = optimizer_Disc_B,Tensor = Tensor, lambda_id = hp.lambda_id)"
      ],
      "metadata": {
        "id": "anKq53pIhHqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Замерим качество модели, обученной на 50 эпохах с помощью метрики FID (Fréchet Inception Distance)"
      ],
      "metadata": {
        "id": "afvOdH-T-sKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "OjGYZxdmJBY4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GeneratorResNet((3,256,256), 9)\n",
        "model.load_state_dict(torch.load('cycle.pth', map_location=device))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "33oSRPF7-riO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images(model, dataloader, output_path, device):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(dataloader)):\n",
        "            real_A = batch['B'].to(device)  # Входные изображения\n",
        "            generated_B = model(real_A)  # Генерация изображений\n",
        "\n",
        "            generated_image = (generated_B * 0.5 + 0.5).cpu().clamp(0, 1)  # Масштабируем к [0, 1]\n",
        "            generated_image = transforms.ToPILImage()(generated_image.squeeze(0))\n",
        "            generated_image.save(f\"{output_path}/generated_{i}.png\")\n",
        "\n",
        "def save_vangogh_images(dataloader, output_path):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    for i, batch in enumerate(tqdm(dataloader)):\n",
        "        real_A = batch['A']\n",
        "        real_A = (real_A * 0.5 + 0.5).cpu().clamp(0, 1)\n",
        "        real_image = transforms.ToPILImage()(real_A.squeeze(0))\n",
        "        real_image.save(f\"{output_path}/real_{i}.png\")"
      ],
      "metadata": {
        "id": "se7nva9LYBzV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(\n",
        "    ImageDataset(root= [hp.val_data_dir_A, hp.val_data_dir_B], transforms_=val_transforms_),\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2)\n",
        "\n",
        "save_vangogh_images(val_loader, 'vangogh_images1')\n",
        "generate_images(model, val_loader, \"generated_images\", device)"
      ],
      "metadata": {
        "id": "Ts47lTqn-87B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fid_value = fid_score.calculate_fid_given_paths([hp.data_dir_A, \"generated_images\"], batch_size=8, device=device, dims=2048)\n",
        "\n",
        "print(f\"FID on vangogh: {fid_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNE-xDJLHJWN",
        "outputId": "404aabdf-9510-41be-e228-26043c55545a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:02<00:00, 16.92it/s]\n",
            "100%|██████████| 94/94 [00:04<00:00, 23.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID on vangogh: 134.88557965121998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fid_value = fid_score.calculate_fid_given_paths([hp.data_dir_B, \"generated_images\"], batch_size=4, device=device, dims=2048)\n",
        "\n",
        "print(f\"FID on real: {fid_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4d-nOvjkeoD",
        "outputId": "7be99e20-db7d-49b7-bfdd-82b4ae15d988"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1572/1572 [00:39<00:00, 40.09it/s]\n",
            "100%|██████████| 188/188 [00:04<00:00, 42.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID on real: 98.51018236805703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Как видно,метрика FID достаточно велика, что говорит об обучении на недостаточном количестве эпох, поэтому необохдимо будет в будущем продолжать дообучать модель до хотя бы 150-200 эпох\n",
        "\n",
        "### расстояние до картин Ван Гога большое, сокроее всего это из-за малого количества картин, а вот расстояние до реальных фото заметно меньше, что говорит о том, что CycleGAN все же выдаетдостаточно реалистичные картины, в отличие от Алгоритма Гатиса, который имеет большее значение FID на рельных фото, то есть больше вносит \"художественный стиль\""
      ],
      "metadata": {
        "id": "dm0CdGLKWs7Z"
      }
    }
  ]
}